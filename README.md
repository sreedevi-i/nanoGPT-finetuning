# nanoGPT-finetuning
A reproducible nanoGPT hyperparameter study on Tiny Shakespeareâ€”128 config sweeps with DDP-ready training, loss/time logging, best-run analysis, and sampling utilities.
